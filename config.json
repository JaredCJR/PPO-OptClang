{
  "_comment":         {
               "These are comments for the following parameters": "",
               "GAMMA": "the decaying rate for rewards;",
               "ClippingEpsilon": "the epsilon in PPO algorithm;",
               "A_LR": "learning rate for actor;",
               "C_LR": "learning rate for critic;",
               "LR_DECAY": "learning rate decay for every N updates(NOT episodes);",
               "LR_DECAY_FREQ": "learning rate decay frequency for every LR_DECAY_FREQ updates",
               "UpdateDepth": "Due to RL may converge very slow, update multiple times for a batch of data.",
               "L1Neurons": "the number of neurons in hidden layer 1;",
               "L2Neurons": "the number of neurons in hidden layer 2; 0 for no layer 2"
                      },
  "WorkerParameters": {
              "EP_MAX"         :100000,
              "N_WORKER"       :10,
              "MIN_BATCH_SIZE" :16,
              "GAMMA"          :0.95
                      },
  "RL_Parameters":    {
              "ClippingEpsilon":0.3,
              "A_LR"           :0.0001,
              "C_LR"           :0.0001,
              "LR_DECAY"       :0.9,
              "LR_DECAY_FREQ"  :3000,
              "UpdateDepth"    :3,
              "L1Neurons"      :512,
              "L2Neurons"      :128
                      }
}
